{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "493c36e8",
   "metadata": {},
   "source": [
    "# Image to Sphere Via Induced Represenations For Pose Estimation\n",
    "\n",
    "This notebook introduces induced image to sphere (which we call Induced_I2S), for the orientation estimation step of single view pose prediction problems. Induced_I2S is based on the Image to Sphere (https://openreview.net/forum?id=_2bDpAtr7PI) architecture.\n",
    "There are a few fundamental differences between I2S and Induced_I2S, which we enumerate here:\n",
    "1. Instead of the orthographic projection used in I2S, Induced_I2S uses a fully differentiable induction layer, which accepts a c-channeled image and outputs a set of matrix valued spherical harmonic coefficients. The orthographic projection is a specific instance of the induction layer. Unlike the orthographic projection, the induction layer creates a signal that has non-zero support everywhere on the sphere.\n",
    "\n",
    "\n",
    "2. The SO(3)-convolution is performed using the method of Saro et al. (https://arxiv.org/abs/2302.03655) which reduces the computational cost from L^{6} to L^{3} where L is the maximum total angular momentum. This signicantly reduces the computational cost of an SO(3) convolution. (To be implemented)\n",
    "\n",
    "3. The is really no a priori reason why we need to induce from the plane to the sphere. We can also use the induced representation to map directly from the plane directly into SO(3).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5e46c5",
   "metadata": {},
   "source": [
    "# Conceptual Questions: To Do List:\n",
    "1. Conceptual question: What should s2 representation be? Re-read spherical CNN paper as this discusses choosing optimal convolutions\n",
    "2. Really need to include pyramid features to deal with discretization error. What is the best way to do this?\n",
    "Specifically, we need to include both low resolution and high resolution discretization\n",
    "3. There is no obvious reason why the sphere is needed. Can potentially go directly to SO(3)\n",
    "4. What non-linearities should be put after induction layer? I.e. What non-linearities are commuting with induction map?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a61e60c",
   "metadata": {},
   "source": [
    "# Implementation: To Do List:\n",
    "1. Put in the induction directly from plane to SO(3). The matrix coeficents satisfy different set of equations\n",
    "2. Implement the Saro Convolution Method\n",
    "3. Implement a multiscale archeteture. FPN arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a878884e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### import relevent packages\n",
    "import torch\n",
    "import numpy as np\n",
    "from e2cnn import gspaces\n",
    "from e2cnn import nn\n",
    "from e2cnn import group\n",
    "from e3nn import o3\n",
    "import e3nn\n",
    "import represenations_opps as rep_ops\n",
    "import healpy as hp\n",
    "\n",
    "import time\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "###import pickle5 as pickle\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef13d03",
   "metadata": {},
   "source": [
    "Check for Avalible GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8df61bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of GPUs avalible: 0\n"
     ]
    }
   ],
   "source": [
    "### check cuda read\n",
    "torch.cuda.is_available()\n",
    "\n",
    "### use gpus if avalible, otherwise use cpu\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "### number of gpus avalible\n",
    "num_gpu = torch.cuda.device_count() \n",
    "\n",
    "print(\"Number of GPUs avalible:\", num_gpu )\n",
    "\n",
    "### get names of gpus if avalible\n",
    "#name = torch.cuda.get_device_name(0)\n",
    "#print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b273f19",
   "metadata": {},
   "source": [
    "# Defining an SO(2) Convolution Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc550bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### defining a SO2 convolution layer\n",
    "SO2_act = gspaces.Rot2dOnR2(N=-1,maximum_frequency=50)\n",
    "class SO2_Convolution_Layer(torch.nn.Module):\n",
    "    ''' SO2 convoluations\n",
    "    :channels: Number of channels in image\n",
    "    : image_shape : images must be square!\n",
    "    : lmax maximum : maximum degree of SO(2) harmonics\n",
    "    : rep_in  : input SO2 representation as list\n",
    "    : rep_out : output SO2 representation as list\n",
    "    '''\n",
    "    def __init__(self, rep_in:list , rep_out:list , k_max:int , kernel_size:int ):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.maximum_frequency = k_max\n",
    "        \n",
    "        ### Defining the SO(2) action on R^{2}\n",
    "        ### set maximum frequency to 50\n",
    "        SO2_act = gspaces.Rot2dOnR2(N=-1,maximum_frequency=self.maximum_frequency)\n",
    "    \n",
    "        ### input and output representations\n",
    "        self.rep_in = rep_in\n",
    "        self.rep_out = rep_out\n",
    "        \n",
    "        ### input and output feature types\n",
    "        self.feat_type_in = nn.FieldType( SO2_act, self.rep_in  )\n",
    "        self.feat_type_out = nn.FieldType( SO2_act, self.rep_out  )\n",
    "        \n",
    "        ### convolution\n",
    "        self.conv = nn.R2Conv( self.feat_type_in, self.feat_type_out, kernel_size=self.kernel_size )\n",
    "        \n",
    "        ### non-linearity\n",
    "        self.non_lin = nn.NormNonLinearity(self.feat_type_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.non_lin( self.conv(x) )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6ac4932",
   "metadata": {},
   "outputs": [],
   "source": [
    "### convert SO2 reps to e2cnn format\n",
    "### This should be put in seperate script\n",
    "def convert_SO2(  input_rep_dict ):\n",
    "    total_rep = []\n",
    "    for k in input_rep_dict.keys():\n",
    "        mulplicites = input_rep_dict[k]        \n",
    "        total_rep = total_rep + mulplicites * [SO2_act.irrep(int(k))]\n",
    "\n",
    "    return total_rep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dca4ca",
   "metadata": {},
   "source": [
    "# Image2Sphere Orthographic Projection Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30ee0461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def s2_healpix_grid(rec_level: int=0, max_beta: float=np.pi/6):\n",
    "    \"\"\"Returns healpix grid up to a max_beta\n",
    "    \"\"\"\n",
    "    n_side = 2**rec_level\n",
    "    npix = hp.nside2npix(n_side)\n",
    "    m = hp.query_disc(nside=n_side, vec=(0,0,1), radius=max_beta)\n",
    "    beta, alpha = hp.pix2ang(n_side, m)\n",
    "    alpha = torch.from_numpy(alpha)\n",
    "    beta = torch.from_numpy(beta)\n",
    "    return torch.stack((alpha, beta)).float()\n",
    "\n",
    "### image 2 sphere orthographic projection\n",
    "class Image2SphereProjector(torch.nn.Module):\n",
    "  \n",
    "    def __init__(self,\n",
    "               fmap_shape, \n",
    "               sphere_fdim: int,\n",
    "               lmax: int,\n",
    "               coverage: float = 0.9,\n",
    "               sigma: float = 0.2,\n",
    "               max_beta: float = np.radians(90),\n",
    "               taper_beta: float = np.radians(75),\n",
    "               rec_level: int = 2,\n",
    "               n_subset: int = 20,\n",
    "              ):\n",
    "        super().__init__()\n",
    "        self.lmax = lmax\n",
    "        self.n_subset = n_subset\n",
    "\n",
    "        # point-wise linear operation to convert to proper dimensionality if needed\n",
    "        if fmap_shape[0] != sphere_fdim:\n",
    "          self.conv1x1 = torch.nn.Conv2d(fmap_shape[0], sphere_fdim, 1)\n",
    "        else:\n",
    "          self.conv1x1 = torch.nn.Identity()\n",
    "\n",
    "        # determine sampling locations for orthographic projection\n",
    "        self.kernel_grid = s2_healpix_grid(max_beta=max_beta, rec_level=rec_level)\n",
    "        self.xyz = o3.angles_to_xyz(*self.kernel_grid)\n",
    "\n",
    "        # orthographic projection\n",
    "        max_radius = torch.linalg.norm(self.xyz[:,[0,2]], dim=1).max()\n",
    "        sample_x = coverage * self.xyz[:,2] / max_radius # range -1 to 1\n",
    "        sample_y = coverage * self.xyz[:,0] / max_radius\n",
    "\n",
    "        gridx, gridy = torch.meshgrid(2*[torch.linspace(-1, 1, fmap_shape[1])], indexing='ij')\n",
    "        scale = 1 / np.sqrt(2 * np.pi * sigma**2)\n",
    "        data = scale * torch.exp(-((gridx.unsqueeze(-1) - sample_x).pow(2) \\\n",
    "                                    +(gridy.unsqueeze(-1) - sample_y).pow(2)) / (2*sigma**2) )\n",
    "        data = data / data.sum((0,1), keepdims=True)\n",
    "\n",
    "        # apply mask to taper magnitude near border if desired\n",
    "        betas = self.kernel_grid[1]\n",
    "        if taper_beta < max_beta:\n",
    "            mask = ((betas - max_beta)/(taper_beta - max_beta)).clamp(max=1).view(1, 1, -1)\n",
    "        else:\n",
    "            mask = torch.ones_like(data)\n",
    "\n",
    "        data = (mask * data).unsqueeze(0).unsqueeze(0).to(torch.float32)\n",
    "        self.weight = torch.nn.Parameter(data= data, requires_grad=True)\n",
    "\n",
    "        self.n_pts = self.weight.shape[-1]\n",
    "        self.ind = torch.arange(self.n_pts)\n",
    "\n",
    "        self.register_buffer(\n",
    "            \"Y\", o3.spherical_harmonics_alpha_beta(range(lmax+1), *self.kernel_grid, normalization='component')\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        :x: float tensor of shape (B, C, H, W)\n",
    "        :return: feature vector of shape (B,P,C) where P is number of points on S2\n",
    "        '''\n",
    "        \n",
    "        #### x.tensor\n",
    "        x = self.conv1x1(x)\n",
    "\n",
    "        if self.n_subset is not None:\n",
    "            self.ind = torch.randperm(self.n_pts)[:self.n_subset]\n",
    "\n",
    "        x = (x.unsqueeze(-1) * self.weight[..., self.ind]).sum((2,3))\n",
    "        x = torch.relu(x)\n",
    "        x = torch.einsum('ni,xyn->xyi', self.Y[self.ind], x) / self.ind.shape[0]**0.5\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda6034a",
   "metadata": {},
   "source": [
    "# Defining an SO(2) to SO(3) Induction Layer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0d680cee",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (3575889141.py, line 103)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[6], line 103\u001b[0;36m\u001b[0m\n\u001b[0;31m    l_tensor = torch.cat(l_coefs,dim=1,2)\u001b[0m\n\u001b[0m                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "### defining an induction layer from SO(2) to SO(3)\n",
    "class Group_Induction_Layer_I(torch.nn.Module):\n",
    "    \n",
    "    ''' The Induction Layer is a Linear Layer that takes an SO2 represetation and outputs SO3 representations\n",
    "    For more details on the induction layer, please read the attached notes!\n",
    "    \n",
    "    Class Induction_Layer teturns matrix valued coefficients of spherical harmonics\n",
    "    :channels: Number of channels in image\n",
    "    :image_shape: integer, images must be square!!!\n",
    "    : kmax: maximum degree of so2 harmonics\n",
    "    :lmax: maximum degree of so3 harmonics\n",
    "    : rep_in  : input SO2 representation as dict\n",
    "    : rep_out : output SO3 representation as dict\n",
    "    \n",
    "    '''\n",
    "\n",
    "    def __init__(self, channels:int , image_shape:int , k_max:int , L_max: int, dict_rep_in :dict , dict_rep_out:dict ):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.k_max = k_max\n",
    "        self.lmax = L_max\n",
    "        self.channels = channels\n",
    "        self.image_shape = image_shape\n",
    "        \n",
    "        ### set of all convolutional parameters as a list\n",
    "        self.convs = torch.nn.ParameterList( [] )\n",
    "    \n",
    "        ### tensor product represenations as list\n",
    "        self.tensor_reps = []\n",
    "        \n",
    "        ### input and output representations as dict\n",
    "        self.dict_rep_in = dict_rep_in\n",
    "        self.dict_rep_out = dict_rep_out\n",
    "        \n",
    "        ###input and output space\n",
    "        self.d_in = rep_ops.compute_SO2_dimension(  self.dict_rep_in )\n",
    "        self.d_out = rep_ops.compute_SO3_dimension(  self.dict_rep_out )\n",
    "        \n",
    "        ### defining SO2 action\n",
    "        SO2_act = gspaces.Rot2dOnR2(N=-1,maximum_frequency=self.k_max)\n",
    "        \n",
    "\n",
    "        ### Defining tensor product input and output features\n",
    "        for l in range( 0 , L_max + 1 ):\n",
    "            \n",
    "            ### output type of so3 rep\n",
    "            tensor_rep_out = rep_ops.compute_tensor_SO3_l_fold( self.dict_rep_out , l )\n",
    "            restrict = rep_ops.compute_restriction_SO3( tensor_rep_out )\n",
    "            rep_out = convert_SO2(  restrict  )            \n",
    "            \n",
    "            ### compute the input so2 rep\n",
    "            tensor_rep_in = rep_ops.compute_tensor_SO2_l_fold( self.dict_rep_in , l )\n",
    "            rep_in = convert_SO2( tensor_rep_in )\n",
    "            \n",
    "            d_in = rep_ops.compute_SO2_dimension( tensor_rep_in )\n",
    "\n",
    "            d_out_so3 = rep_ops.compute_SO3_dimension(  tensor_rep_out ) \n",
    "            d_out_so2 = rep_ops.compute_SO2_dimension(  restrict )\n",
    "            \n",
    "            ### print(d_in  , d_out_so2, d_out_so3 )\n",
    "            \n",
    "            ###feature types\n",
    "            self.feat_type_in = nn.FieldType( SO2_act, rep_in  )\n",
    "            self.feat_type_out = nn.FieldType( SO2_act, rep_out  )\n",
    "            \n",
    "            ### convs filters\n",
    "            conv = nn.R2Conv( self.feat_type_in, self.feat_type_out, kernel_size=self.image_shape)\n",
    "            self.convs.append( conv )   \n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        ### convert x to a tensor if x is geometric tensor\n",
    "        x = x.tensor\n",
    "        \n",
    "        ### compute the l-th spherical harmonic matrix coeficent\n",
    "        l_coefs = []\n",
    "        for l in range( 0 ,  self.lmax  ):\n",
    "            \n",
    "            ## get filters\n",
    "            F_val = self.convs[l].expand_parameters()[0]\n",
    "            \n",
    "            ### print( F_val.shape , (l+1)*self.d_out , (l+1)*self.d_in )\n",
    "            \n",
    "            ###input split and chunk\n",
    "            F_val_cat = torch.split( F_val , split_size_or_sections= self.d_out , dim=0)\n",
    "            g_split = torch.stack( F_val_cat ,dim=0 )\n",
    "            \n",
    "            #### output split and chunk\n",
    "            g_split_full = torch.split( g_split , split_size_or_sections= self.d_in , dim=2)\n",
    "            g_split_done = torch.stack( g_split_full ,dim=1 )\n",
    "            \n",
    "         \n",
    "            out = torch.einsum('ijklmn , almn-> aijk',  g_split_done , x )\n",
    "            \n",
    "            print(out.shape)\n",
    "            \n",
    "            l_coefs.append( out )\n",
    "        \n",
    "        \n",
    "      \n",
    "        \n",
    "        l_tensor = torch.cat(l_coefs,dim=1)\n",
    "            \n",
    "            \n",
    "        return l_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "40874378",
   "metadata": {},
   "outputs": [],
   "source": [
    "### defining an induction layer from SO(2) to SO(3)\n",
    "class SO3_Induction_Layer(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_shape: tuple , sphere_fdim: int ):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.sphere_fdim = sphere_fdim\n",
    "        self.k_max = 8\n",
    "        self.lmax = lmax + 1\n",
    "        self.channels = input_shape[0]\n",
    "        self.image_shape = input_shape[1]\n",
    "        \n",
    "        ###dropout probility\n",
    "        self.p = 0.1\n",
    "        \n",
    "        ###input is just many copies of trival\n",
    "        dict_rep_in = { '0' : self.channels }\n",
    "\n",
    "\n",
    "        #### the output rep\n",
    "        ### this can be anything with self.sphere_fdim dimension\n",
    "        ### consider changing to be less dimension\n",
    "        dict_rep_out = { '0' : self.sphere_fdim }\n",
    "        \n",
    "        ### set of all convolutional parameters as a list\n",
    "        self.convs = torch.nn.ParameterList( [] )\n",
    "    \n",
    "        ### tensor product represenations as list\n",
    "        self.tensor_reps = []\n",
    "        \n",
    "        ### input and output representations as dict\n",
    "        self.dict_rep_in = dict_rep_in\n",
    "        self.dict_rep_out = dict_rep_out\n",
    "        \n",
    "        ### input and output space dimensions\n",
    "        self.d_in = rep_ops.compute_SO2_dimension(  self.dict_rep_in )\n",
    "        self.d_out = rep_ops.compute_SO3_dimension(  self.dict_rep_out )\n",
    "        \n",
    "        ### print( self.d_in , self.d_out )\n",
    "        \n",
    "        ### defining the SO2 action\n",
    "        SO2_act = gspaces.Rot2dOnR2(N=-1,maximum_frequency=self.k_max)\n",
    "        \n",
    "        ### Defining tensor product input and output features\n",
    "        for l in range( 0 , self.lmax ):\n",
    "            \n",
    "            ### output type of so3 rep\n",
    "            tensor_rep_out = rep_ops.compute_tensor_SO3_l_fold( self.dict_rep_out , l )\n",
    "            restrict = rep_ops.compute_restriction_SO3( tensor_rep_out )\n",
    "            rep_out = convert_SO2(  restrict  )            \n",
    "            \n",
    "            ### compute the input so2 rep\n",
    "            tensor_rep_in = rep_ops.compute_tensor_SO2_l_fold( self.dict_rep_in , l )\n",
    "            rep_in = convert_SO2( tensor_rep_in )\n",
    "            \n",
    "            d_in = rep_ops.compute_SO2_dimension( tensor_rep_in )\n",
    "\n",
    "            d_out_so3 = rep_ops.compute_SO3_dimension(  tensor_rep_out ) \n",
    "            d_out_so2 = rep_ops.compute_SO2_dimension(  restrict )\n",
    "            \n",
    "            print(d_in  , d_out_so2, d_out_so3 )\n",
    "            \n",
    "            ###feature types\n",
    "            self.feat_type_in = nn.FieldType( SO2_act, rep_in  )\n",
    "            self.feat_type_out = nn.FieldType( SO2_act, rep_out  )\n",
    "            \n",
    "            ### convs filters\n",
    "            conv = nn.R2Conv( self.feat_type_in, self.feat_type_out, kernel_size=self.image_shape)\n",
    "            self.convs.append( conv )   \n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        ### convert x to a tensor if x is geometric tensor\n",
    "        x = x.tensor\n",
    "        \n",
    "        ### compute the l-th spherical harmonic matrix coeficent\n",
    "        l_coefs = []\n",
    "        for l in range( 0 ,  self.lmax  ):\n",
    "            \n",
    "            ## get filters\n",
    "            F_val = self.convs[l].expand_parameters()[0]\n",
    "            \n",
    "            ### print( F_val.shape , (l+1)*self.d_out , (l+1)*self.d_in )\n",
    "            \n",
    "            ###input split and chunk\n",
    "            F_val_cat = torch.split( F_val , split_size_or_sections= self.d_out , dim=0)\n",
    "            g_split = torch.stack( F_val_cat ,dim=0 )\n",
    "            \n",
    "            #### output split and chunk\n",
    "            g_split_full = torch.split( g_split , split_size_or_sections= self.d_in , dim=2)\n",
    "            g_split_done = torch.stack( g_split_full ,dim=1 )\n",
    "            \n",
    "         \n",
    "            out = torch.einsum('ijklmn , almn-> aijk',  g_split_done , x )\n",
    "            \n",
    "            print(out.shape)\n",
    "            \n",
    "            l_coefs.append( out )\n",
    "        \n",
    "        \n",
    "      \n",
    "        \n",
    "        l_tensor = torch.cat(l_coefs,dim=1)\n",
    "            \n",
    "            \n",
    "        return l_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5a2735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210 15 15\n",
      "630 45 45\n",
      "1050 75 75\n",
      "1470 105 105\n"
     ]
    }
   ],
   "source": [
    "rep_in = [ SO2_act.irrep(0) ]\n",
    "feat_type_in = nn.FieldType( SO2_act, rep_in  )\n",
    "\n",
    " # ### specifiy the hiden SO2 layer muplicities\n",
    "hidden_mulplicities_SO2 = { '0' : 1 , '1': 1 , '2':1 , '3':1   }\n",
    "hidden_so2 = convert_SO2( hidden_mulplicities_SO2 )\n",
    "\n",
    "kmax = 20\n",
    "lmax = 6\n",
    "kernel_size = 3\n",
    "channels_in = 210\n",
    "image_size = 10\n",
    "\n",
    "batch_size = 5\n",
    "### SO2 convolution layer\n",
    "SO2_conv_1 = SO2_Convolution_Layer( rep_in = rep_in, rep_out = hidden_so2 ,k_max = kmax,  kernel_size= kernel_size )\n",
    "\n",
    "# ### the output mupliciteis of the induced SO3 layer\n",
    "mulplicities_SO3 = { '0' :1 , '1' : 1 , '2' : 1   }\n",
    "\n",
    "# ##### the induction representation layer, \n",
    "# ### compute the number of output channels of hidden rep\n",
    "## channels_in = rep_ops.compute_SO2_dimension( hidden_mulplicities_SO2  )\n",
    "\n",
    "### first induction layer method\n",
    "## induce_I = Group_Induction_Layer_I( channels = channels_in, image_shape=(image_size - kernel_size +1) , k_max =kmax, L_max=lmax , dict_rep_in = hidden_mulplicities_SO2 , dict_rep_out = mulplicities_SO3 )\n",
    "input_shape = [channels_in,image_size]\n",
    "so3_induce = SO3_Induction_Layer( input_shape = input_shape , sphere_fdim = 15  )\n",
    "\n",
    "print(so3_induce)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c11af05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[5.2040e-03, 9.8159e-01, 5.5502e-01, 6.5770e-01, 4.9620e-01,\n",
      "           8.5580e-01, 3.4187e-02, 3.5910e-01, 8.8188e-01, 5.6823e-01],\n",
      "          [2.7988e-01, 8.7438e-01, 1.1401e-01, 3.2496e-01, 2.2658e-02,\n",
      "           7.9548e-01, 5.5532e-01, 9.8161e-01, 1.9070e-02, 9.6225e-01],\n",
      "          [6.7182e-01, 4.2635e-01, 2.7867e-01, 8.3270e-01, 1.3461e-01,\n",
      "           9.0029e-01, 9.0362e-01, 5.0239e-01, 6.7894e-02, 4.5175e-01],\n",
      "          [8.8160e-01, 1.9020e-02, 6.2831e-01, 6.6655e-01, 7.2490e-02,\n",
      "           4.5307e-01, 7.7476e-02, 7.9424e-01, 4.9250e-01, 3.3369e-01],\n",
      "          [5.2850e-01, 6.9901e-01, 7.9288e-01, 6.4836e-01, 3.4757e-01,\n",
      "           8.0347e-01, 8.4256e-01, 5.5067e-01, 3.8257e-02, 3.1057e-01],\n",
      "          [3.0751e-01, 1.4018e-01, 3.2517e-01, 7.1919e-01, 9.8683e-02,\n",
      "           7.9049e-01, 9.2760e-02, 6.0196e-02, 1.6074e-01, 2.5648e-01],\n",
      "          [2.3387e-01, 8.7683e-01, 7.5615e-01, 8.2100e-01, 6.5694e-01,\n",
      "           2.0350e-01, 5.1654e-02, 7.5868e-01, 4.9637e-01, 3.2813e-01],\n",
      "          [7.2142e-01, 6.2505e-01, 5.4982e-01, 7.5359e-02, 4.6808e-01,\n",
      "           5.4747e-01, 7.2354e-01, 5.3355e-01, 8.9491e-01, 9.7742e-01],\n",
      "          [2.5931e-02, 6.7771e-01, 2.1125e-01, 1.8803e-01, 3.2124e-01,\n",
      "           2.1266e-01, 7.6109e-01, 5.5253e-01, 7.8248e-02, 8.0405e-01],\n",
      "          [5.0429e-02, 6.9253e-01, 5.9437e-01, 3.7730e-01, 2.0419e-01,\n",
      "           9.4938e-01, 1.1401e-01, 7.2118e-01, 6.4533e-03, 4.9002e-01]]],\n",
      "\n",
      "\n",
      "        [[[7.0651e-01, 7.6000e-01, 4.7451e-01, 2.2706e-02, 9.1246e-01,\n",
      "           9.1806e-01, 2.5140e-02, 1.1933e-01, 2.5484e-01, 9.2648e-01],\n",
      "          [9.2230e-01, 3.7175e-02, 6.3247e-01, 3.2586e-01, 8.7093e-01,\n",
      "           3.8864e-02, 8.4731e-01, 5.4425e-01, 4.0480e-01, 8.5688e-01],\n",
      "          [3.5185e-01, 9.1735e-01, 6.6857e-01, 9.7294e-01, 6.2113e-01,\n",
      "           4.4215e-02, 5.8507e-03, 3.0603e-01, 7.2198e-01, 2.4830e-01],\n",
      "          [9.3010e-01, 5.3823e-01, 7.7074e-01, 1.4232e-01, 2.5984e-01,\n",
      "           4.7033e-01, 1.2101e-01, 2.7005e-01, 8.6995e-01, 7.4693e-01],\n",
      "          [4.9638e-01, 6.4058e-01, 5.1971e-01, 5.8623e-01, 5.1080e-01,\n",
      "           9.6039e-01, 4.2108e-01, 8.8453e-01, 3.7857e-02, 8.9755e-01],\n",
      "          [3.9465e-01, 7.0266e-01, 1.2719e-01, 3.6185e-01, 6.0644e-01,\n",
      "           1.0344e-01, 3.0919e-01, 9.2407e-01, 1.7135e-01, 3.7703e-01],\n",
      "          [9.3198e-01, 6.9255e-01, 1.3666e-01, 9.3530e-02, 5.5003e-01,\n",
      "           2.8652e-01, 8.5770e-01, 4.4027e-01, 3.8670e-01, 9.9956e-01],\n",
      "          [4.4669e-01, 6.1958e-01, 2.9825e-01, 5.3206e-01, 3.1684e-01,\n",
      "           9.4192e-01, 5.7336e-02, 5.3023e-01, 4.2915e-01, 7.5279e-01],\n",
      "          [3.6833e-01, 7.9598e-01, 3.1415e-02, 4.8519e-01, 7.3968e-01,\n",
      "           6.4715e-01, 3.5226e-01, 8.4352e-01, 6.7265e-01, 2.6446e-01],\n",
      "          [8.3469e-01, 3.5948e-01, 8.1506e-01, 3.3411e-01, 6.9933e-02,\n",
      "           5.9111e-01, 5.7737e-01, 6.1888e-01, 6.7084e-01, 9.3682e-01]]],\n",
      "\n",
      "\n",
      "        [[[2.7577e-01, 9.9614e-01, 7.5967e-01, 5.6650e-01, 3.8071e-01,\n",
      "           7.4907e-01, 6.7557e-02, 7.2199e-01, 8.0677e-01, 1.3739e-03],\n",
      "          [6.7701e-01, 5.9158e-01, 3.3523e-01, 1.6753e-01, 1.6596e-01,\n",
      "           1.3659e-01, 9.8183e-01, 2.5149e-01, 7.9279e-01, 9.7850e-01],\n",
      "          [8.0292e-01, 5.9894e-02, 2.2529e-01, 2.5094e-01, 8.7975e-01,\n",
      "           6.8327e-01, 6.1188e-01, 7.6173e-01, 3.0498e-01, 8.8333e-01],\n",
      "          [6.9988e-01, 4.8982e-01, 5.4106e-01, 5.7261e-01, 1.3036e-01,\n",
      "           7.3388e-01, 4.3850e-01, 9.3548e-01, 6.5603e-01, 6.6862e-01],\n",
      "          [5.8843e-01, 7.6095e-01, 9.6143e-02, 9.7159e-01, 2.1540e-01,\n",
      "           4.2783e-01, 4.5370e-02, 7.6836e-01, 5.8314e-03, 4.5090e-01],\n",
      "          [9.2453e-01, 4.1976e-01, 5.9087e-01, 3.7707e-02, 4.2290e-01,\n",
      "           3.0525e-01, 4.9508e-01, 2.9199e-01, 7.8567e-01, 8.3912e-01],\n",
      "          [3.2808e-01, 1.1504e-01, 9.6543e-01, 9.9195e-01, 5.1631e-01,\n",
      "           1.1566e-01, 4.3198e-01, 5.8996e-01, 1.2911e-01, 4.6691e-01],\n",
      "          [5.0626e-01, 1.2141e-01, 4.8328e-01, 5.6850e-01, 1.6175e-01,\n",
      "           9.0924e-02, 3.1754e-01, 7.0994e-01, 7.8745e-01, 7.5606e-01],\n",
      "          [4.2889e-01, 5.3215e-01, 1.7568e-01, 1.9653e-01, 5.4573e-01,\n",
      "           1.2043e-01, 6.5641e-01, 7.0804e-01, 5.7158e-01, 2.6424e-01],\n",
      "          [2.5195e-01, 5.2155e-02, 8.1044e-01, 6.9770e-01, 6.9929e-01,\n",
      "           5.5022e-01, 9.7933e-01, 8.5719e-01, 5.6161e-01, 9.7806e-01]]],\n",
      "\n",
      "\n",
      "        [[[7.6906e-01, 1.9694e-01, 2.1156e-01, 9.4181e-01, 2.3698e-01,\n",
      "           3.3237e-01, 5.9102e-01, 6.5136e-01, 9.4243e-01, 1.3592e-02],\n",
      "          [9.6903e-02, 9.2765e-01, 7.3595e-01, 1.2340e-01, 3.0100e-01,\n",
      "           3.8731e-01, 6.7110e-01, 4.7398e-02, 1.9196e-01, 5.8790e-01],\n",
      "          [6.4620e-02, 6.8464e-02, 4.1257e-01, 2.2138e-01, 6.3382e-01,\n",
      "           9.6470e-01, 1.4703e-01, 7.2987e-01, 9.9567e-01, 4.3112e-01],\n",
      "          [4.6909e-01, 3.8987e-01, 9.4869e-01, 1.8254e-01, 5.1412e-01,\n",
      "           4.5239e-01, 1.9657e-01, 2.9558e-04, 3.4119e-01, 8.7623e-02],\n",
      "          [4.8585e-01, 4.3160e-01, 8.6599e-01, 6.4009e-01, 8.1823e-01,\n",
      "           2.4354e-01, 6.3375e-01, 4.5785e-01, 7.4509e-01, 6.2680e-01],\n",
      "          [1.3688e-01, 8.3734e-02, 7.4512e-01, 4.9275e-01, 9.6930e-01,\n",
      "           8.7937e-01, 4.5212e-01, 6.0569e-01, 4.1353e-01, 5.7529e-02],\n",
      "          [7.0115e-01, 7.6652e-01, 4.6230e-01, 7.6721e-01, 3.3922e-01,\n",
      "           1.9763e-01, 8.6695e-01, 1.9470e-01, 2.8028e-01, 4.5085e-01],\n",
      "          [7.8580e-01, 3.3738e-01, 8.7360e-01, 1.9091e-01, 4.0503e-01,\n",
      "           9.4088e-01, 3.5137e-01, 4.2183e-01, 3.3721e-01, 2.3949e-01],\n",
      "          [5.4531e-01, 1.9228e-01, 9.3899e-01, 5.3272e-01, 9.0400e-01,\n",
      "           8.3438e-01, 5.4344e-01, 1.5424e-01, 8.1269e-01, 2.9900e-01],\n",
      "          [7.8704e-01, 3.9554e-02, 2.9034e-01, 5.8350e-01, 3.9681e-01,\n",
      "           9.7894e-01, 5.5453e-01, 4.2786e-01, 7.9326e-01, 6.2493e-01]]],\n",
      "\n",
      "\n",
      "        [[[9.3993e-01, 5.4841e-01, 6.5897e-01, 2.1023e-01, 5.2144e-01,\n",
      "           8.5391e-01, 9.7562e-01, 7.0433e-01, 3.7384e-02, 9.0508e-02],\n",
      "          [4.0584e-01, 1.2637e-01, 4.3007e-01, 2.4337e-01, 7.7139e-01,\n",
      "           9.7162e-01, 1.7108e-01, 7.8524e-01, 2.4522e-01, 1.3609e-01],\n",
      "          [2.5248e-01, 1.4682e-01, 1.5087e-01, 5.1460e-01, 1.9530e-02,\n",
      "           1.2166e-01, 6.8372e-01, 3.5368e-01, 4.2672e-01, 4.8438e-01],\n",
      "          [6.5029e-01, 9.6329e-01, 4.0160e-01, 3.4985e-01, 6.0340e-01,\n",
      "           7.6142e-01, 1.3446e-01, 8.6761e-01, 2.4003e-01, 4.3277e-01],\n",
      "          [1.8811e-01, 1.7856e-01, 3.4058e-01, 3.8774e-02, 3.6049e-01,\n",
      "           1.3753e-01, 3.7910e-01, 2.8923e-01, 7.4649e-01, 6.2027e-01],\n",
      "          [9.3575e-01, 9.4690e-01, 5.8805e-01, 5.4118e-01, 3.6860e-01,\n",
      "           9.1332e-01, 8.1198e-01, 5.8365e-01, 4.9037e-01, 4.0174e-02],\n",
      "          [9.6535e-02, 6.0404e-01, 1.4880e-01, 2.1893e-01, 1.3859e-03,\n",
      "           1.8115e-01, 3.8700e-01, 3.7486e-01, 5.5642e-01, 5.0670e-02],\n",
      "          [2.8204e-01, 4.2884e-01, 6.8760e-01, 2.9512e-01, 3.4648e-01,\n",
      "           5.4951e-01, 5.8587e-01, 2.4300e-01, 8.3949e-02, 1.7387e-01],\n",
      "          [9.6411e-01, 1.5553e-01, 6.5599e-01, 2.5083e-01, 5.2406e-01,\n",
      "           6.0647e-01, 6.7493e-01, 8.1497e-01, 1.9431e-01, 6.7072e-02],\n",
      "          [8.9562e-04, 7.5854e-01, 6.6064e-01, 7.2081e-01, 9.6010e-01,\n",
      "           3.7800e-01, 1.1824e-01, 9.5552e-01, 1.8991e-01, 5.5467e-01]]]])\n",
      "0.0007092952728271484\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "x = torch.rand(batch_size,1,image_size,image_size) #z = induce_I( y )\n",
    "stop_time = time.time()\n",
    "\n",
    "print(x)\n",
    "\n",
    "print( stop_time-start_time )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e13f52b",
   "metadata": {},
   "source": [
    "# Defining an SO(2) to Sphere Induction Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "949d8be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### The naive method: This is wayyyyy too slow\n",
    "### defining an induction layer from SO(2) to SO(3)\n",
    "class Induction_Layer_I(torch.nn.Module):\n",
    "    \n",
    "    ''' The Induction Layer is a Linear Layer that takes an SO2 represetation and outputs SO3 representations\n",
    "    For more details on the induction layer, please read the attached notes!\n",
    "    \n",
    "    Class Induction_Layer teturns matrix valued coefficients of spherical harmonics\n",
    "    :channels: Number of channels in image\n",
    "    :image_shape: integer, images must be square!!!\n",
    "    : kmax: maximum degree of so2 harmonics\n",
    "    :lmax: maximum degree of so3 harmonics\n",
    "    : rep_in  : input SO2 representation as dict\n",
    "    : rep_out : output SO3 representation as dict\n",
    "    \n",
    "    '''\n",
    "\n",
    "    def __init__(self, channels:int , image_shape:int , k_max:int , L_max: int, dict_rep_in :dict , dict_rep_out:dict ):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.k_max = k_max\n",
    "        self.lmax = L_max\n",
    "        self.channels = channels\n",
    "        self.image_shape = image_shape\n",
    "        \n",
    "        ### set of all convolutional parameters as a list\n",
    "        self.convs = torch.nn.ParameterList( [] )\n",
    "    \n",
    "        ### tensor product represenations as list\n",
    "        self.tensor_reps = []\n",
    "        \n",
    "        ### input and output representations as dict\n",
    "        self.dict_rep_in = dict_rep_in\n",
    "        self.dict_rep_out = dict_rep_out\n",
    "        \n",
    "        ### defining SO2 action\n",
    "        SO2_act = gspaces.Rot2dOnR2(N=-1,maximum_frequency=self.k_max)\n",
    "        \n",
    "        ### compute restriction of SO(3) output SO(2) representation\n",
    "        restrict = rep_ops.compute_restriction_SO3( self.dict_rep_out )\n",
    "        self.rep_out = convert_SO2(  restrict  )\n",
    "        \n",
    "        ### output feature types\n",
    "        self.feat_type_out = nn.FieldType( SO2_act, self.rep_out  )\n",
    "        \n",
    "        ### Defining tensor product features\n",
    "        for l in range( 0 , L_max + 1 ):\n",
    "            \n",
    "            tensor_rep_in = rep_ops.compute_tensor_SO2_l_fold( self.dict_rep_in , l )\n",
    "            \n",
    "            \n",
    "            #### compute the\n",
    "            rep_in = convert_SO2( tensor_rep_in )\n",
    "            #print( tensor_rep_in )\n",
    "            #### compute the dimension of an so2 rep\n",
    "            d_v = rep_ops.compute_tensor_SO2_dimension( tensor_rep_in  )\n",
    "            \n",
    "            self.feat_type_in = nn.FieldType( SO2_act, rep_in  )\n",
    "                \n",
    "            conv = nn.R2Conv( self.feat_type_in, self.feat_type_out, kernel_size=self.image_shape)\n",
    "            self.convs.append( conv )   \n",
    "\n",
    "            \n",
    "    def forward(self, x):\n",
    "        \n",
    "        ### convert x to a tensor if x is geometric tensor\n",
    "        x = x.tensor\n",
    "        \n",
    "        ### compute the l-th spherical harmonic matrix coeficent\n",
    "        l_coefs = []\n",
    "        for l in range( 0 ,  self.lmax  ):\n",
    "            \n",
    "            ## get filters\n",
    "            F_val = self.convs[l].expand_parameters()[0]\n",
    "            \n",
    "            l_k_coefs = []\n",
    "            for k in range(2*l+1):\n",
    "                \n",
    "                ### reformat to be same size\n",
    "                F_k = F_val[:,self.channels*k:self.channels*k + self.channels*1,:,:]\n",
    "                out = torch.einsum('ijkl , ajkl-> ai',  F_k , x )\n",
    "                l_k_coefs.append(out)\n",
    "\n",
    "            \n",
    "            l_k_tensor = torch.stack( l_k_coefs )\n",
    "            #l_k_tensor = torch.einsum('ijk -> jki ', l_k_tensor )\n",
    "            \n",
    "            l_coefs.append( l_k_tensor )\n",
    "        \n",
    "        l_tensor = torch.cat(l_coefs,dim=0)\n",
    "        l_tensor = torch.einsum('ijk -> jki ', l_tensor )\n",
    "        \n",
    "        return l_tensor\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f1b3259b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### faster version -- still way to slow\n",
    "### defining an induction layer from SO(2) to SO(3)\n",
    "### aka the Ondrej method\n",
    "class Induction_Layer_II( torch.nn.Module ):\n",
    "    \n",
    "    ''' The Induction Layer is a Linear Layer that takes an SO2 represetation and outputs SO3 representations\n",
    "    For more details on the induction layer, please read the attached notes!\n",
    "    \n",
    "    Class Induction_Layer teturns matrix valued coefficients of spherical harmonics\n",
    "    :channels: Number of channels in image\n",
    "    :image_shape: integer, images must be square!!!\n",
    "    : kmax: maximum degree of so2 harmonics\n",
    "    :lmax: maximum degree of so3 harmonics\n",
    "    : rep_in  : input SO2 representation as dict\n",
    "    : rep_out : output SO3 representation as dict\n",
    "    \n",
    "    '''\n",
    "\n",
    "    def __init__(self, channels:int , image_shape:int , k_max:int , L_max: int, dict_rep_in :dict , dict_rep_out:dict ):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.k_max = k_max\n",
    "        self.lmax = L_max\n",
    "        self.channels = channels\n",
    "        self.image_shape = image_shape\n",
    "        \n",
    "        ### set of all convolutional parameters as a list\n",
    "        self.convs = torch.nn.ParameterList( [] )\n",
    "    \n",
    "        ### tensor product represenations as list\n",
    "        self.tensor_reps = []\n",
    "        \n",
    "        ### input and output representations as dict\n",
    "        self.dict_rep_in = dict_rep_in\n",
    "        self.dict_rep_out = dict_rep_out\n",
    "        \n",
    "        ### defining SO2 action\n",
    "        SO2_act = gspaces.Rot2dOnR2(N=-1,maximum_frequency=self.k_max)\n",
    "        \n",
    "        ### compute restriction of SO(3) output SO(2) representation\n",
    "        restrict = rep_ops.compute_restriction_SO3( self.dict_rep_out )\n",
    "        self.rep_out = convert_SO2(  restrict  )\n",
    "        \n",
    "        ### output feature types\n",
    "        self.feat_type_out = nn.FieldType( SO2_act, self.rep_out  )\n",
    "        \n",
    "        \n",
    "        ### Defining tensor product features\n",
    "        convs_list = []\n",
    "        for l in range( 0 , L_max+1 ):\n",
    "            \n",
    "            tensor_rep_in = rep_ops.compute_tensor_SO2_l_fold( self.dict_rep_in , l )\n",
    "            \n",
    "            rep_in = convert_SO2( tensor_rep_in )\n",
    "            self.d_v = rep_ops.compute_tensor_SO2_dimension( tensor_rep_in  )\n",
    "            self.feat_type_in = nn.FieldType( SO2_act, rep_in  ) \n",
    "            conv = nn.R2Conv( self.feat_type_in, self.feat_type_out, kernel_size=self.image_shape)\n",
    "            self.convs.append( conv )   \n",
    "            \n",
    "        #self.convs = convs_list\n",
    "            \n",
    "    def forward(self, x):\n",
    "        \n",
    "        ### convert x to a tensor if x is geometric tensor\n",
    "        x = x.tensor\n",
    "        \n",
    "        ### There should be faster way to do this...\n",
    "        \n",
    "        ### compute the l-th spherical harmonic matrix coeficent\n",
    "        l_coefs = []\n",
    "        F_val_list = []\n",
    "        for l in range( 0 ,  self.lmax  ):\n",
    "            ## get filters\n",
    "            F_val = self.convs[l].expand_parameters()[0]\n",
    "            F_val_list.append(F_val)\n",
    "            #print(\"F_val.shape\", F_val.shape)\n",
    "        F_val_cat = torch.concat(F_val_list, dim=1)\n",
    "        #print(\"F_val_cat.shape\", F_val_cat.shape)\n",
    "                \n",
    "        ###split and stack\n",
    "        g = torch.split( F_val_cat , split_size_or_sections= self.channels , dim=1)\n",
    "        #print(\"g[0].shape\", g[0].shape)\n",
    "        g_tens = torch.stack( g ,dim=0 )\n",
    "        #print(\"g_tens.shape\", g_tens.shape)\n",
    "\n",
    "        ###comput sum\n",
    "        # torch.Size([23, 36, 5, 100, 100])\n",
    "        test_v = torch.einsum('ijklm , aklm-> iaj',  g_tens , x )\n",
    "        l_tensor = test_v\n",
    "                    \n",
    "        ### cat\n",
    "        # l_tensor = torch.cat(l_coefs,dim=0)    \n",
    "        #print(\"l_tensor\", l_tensor.shape)\n",
    "        l_tensor = l_tensor.swapaxes(0,1).swapaxes(1,2)\n",
    "        \n",
    "        return l_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b17de06",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Different approch, done using just matrix multiplications\n",
    "### This is still slow, but on par with the projection method\n",
    "### defining an induction layer from SO(2) to SO(3)\n",
    "class Induction_Layer_III( torch.nn.Module ):\n",
    "    \n",
    "    ''' The Induction Layer is a Linear Layer that takes an SO2 represetation and outputs SO3 representations\n",
    "    For more details on the induction layer, please read the attached notes!\n",
    "    \n",
    "    Class Induction_Layer teturns matrix valued coefficients of spherical harmonics\n",
    "    :channels: Number of channels in image\n",
    "    :image_shape: integer, images must be square!!!\n",
    "    : kmax: maximum degree of so2 harmonics\n",
    "    :lmax: maximum degree of so3 harmonics\n",
    "    : rep_in  : input SO2 representation as dict\n",
    "    : rep_out : output SO3 representation as dict\n",
    "    \n",
    "    '''\n",
    "\n",
    "    def __init__(self, channels:int , image_shape:int , k_max:int , L_max: int, dict_rep_in :dict , dict_rep_out:dict ):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.k_max = k_max\n",
    "        self.lmax = L_max\n",
    "        self.channels = channels\n",
    "        self.image_shape = image_shape\n",
    "        \n",
    "        ### tensor product represenations as list\n",
    "        self.tensor_reps = []\n",
    "        \n",
    "        ### input and output representations as dict\n",
    "        self.dict_rep_in = dict_rep_in\n",
    "        self.dict_rep_out = dict_rep_out\n",
    "        \n",
    "        ### defining SO2 action\n",
    "        SO2_act = gspaces.Rot2dOnR2(N=-1,maximum_frequency=self.k_max)\n",
    "        \n",
    "        ### compute restriction of SO(3) output SO(2) representation\n",
    "        restrict = rep_ops.compute_restriction_SO3( self.dict_rep_out )\n",
    "        self.rep_out = convert_SO2(  restrict  )\n",
    "        \n",
    "        ### output feature types\n",
    "        self.feat_type_out = nn.FieldType( SO2_act, self.rep_out  )\n",
    "        \n",
    "        \n",
    "        ### compute direct sum representation\n",
    "        total_rep = []\n",
    "        for l in range( 0 , L_max ):\n",
    "            \n",
    "            tensor_rep_in = rep_ops.compute_tensor_SO2_l_fold( self.dict_rep_in , l )\n",
    "            \n",
    "            rep_in = convert_SO2( tensor_rep_in )\n",
    "\n",
    "            \n",
    "            total_rep = total_rep + rep_in\n",
    "            \n",
    "        feat_type_in = nn.FieldType( SO2_act, total_rep  )\n",
    "            \n",
    "        self.conv = nn.R2Conv( feat_type_in , self.feat_type_out , kernel_size=self.image_shape)        \n",
    "        \n",
    "        \n",
    "            \n",
    "    def forward(self, x):\n",
    "                \n",
    "        F = self.conv.expand_parameters()[0]\n",
    "        F_val_cat = torch.split( F , split_size_or_sections= self.channels , dim=1)\n",
    "        g_split = torch.stack( F_val_cat ,dim=0 )\n",
    "            \n",
    "        ###now contract\n",
    "        y = torch.einsum('ijklm , aklm -> aji' ,  g_split , x.tensor )\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40328170",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3484.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "rep_in = [ SO2_act.irrep(0) ]\n",
    "feat_type_in = nn.FieldType( SO2_act, rep_in  )\n",
    "\n",
    " # ### specifiy the hiden SO2 layer muplicities\n",
    "hidden_mulplicities_SO2 = { '0' : 1 , '1': 1 , '2':1 , '3':1 , '4':1   }\n",
    "hidden_so2 = convert_SO2( hidden_mulplicities_SO2 )\n",
    "\n",
    "kmax = 10\n",
    "lmax = 6\n",
    "kernel_size = 20\n",
    "image_size = 200\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "### SO2 convolution layer\n",
    "SO2_conv_1 = SO2_Convolution_Layer( rep_in = rep_in, rep_out = hidden_so2 ,k_max = kmax,  kernel_size= kernel_size )\n",
    "\n",
    "# ### the output mupliciteis of the induced SO3 layer\n",
    "mulplicities_SO3 = { '0' :1 , '1' : 1 , '2' : 1 , '3':1   }\n",
    "\n",
    "# ##### the induction representation layer, \n",
    "# ### compute the number of output channels of hidden rep\n",
    "channels_in = rep_ops.compute_SO2_dimension( hidden_mulplicities_SO2  )\n",
    "\n",
    "### first induction layer method\n",
    "induce_I = Induction_Layer_I( channels = channels_in, image_shape=(image_size - kernel_size +1) , k_max =kmax, L_max=lmax , dict_rep_in = hidden_mulplicities_SO2 , dict_rep_out = mulplicities_SO3 )\n",
    "\n",
    "### second induction layer method\n",
    "induce_II = Induction_Layer_II( channels = channels_in, image_shape=(image_size - kernel_size +1) , k_max =kmax, L_max=lmax , dict_rep_in = hidden_mulplicities_SO2 , dict_rep_out = mulplicities_SO3 )\n",
    "\n",
    "### second induction layer method\n",
    "induce_III = Induction_Layer_III( channels = channels_in, image_shape=(image_size - kernel_size +1) , k_max =kmax, L_max=lmax , dict_rep_in = hidden_mulplicities_SO2 , dict_rep_out = mulplicities_SO3 )\n",
    "\n",
    "\n",
    "\n",
    "### compare with orthographic projection\n",
    "orthographic_proj = Image2SphereProjector( fmap_shape=(9,image_size -kernel_size+1,image_size-kernel_size+1), sphere_fdim= 50, lmax=lmax,\n",
    "               coverage = 0.9,\n",
    "               sigma = 0.2,\n",
    "               max_beta = np.radians(90),\n",
    "               taper_beta = np.radians(75),\n",
    "               rec_level = 2,\n",
    "               n_subset = 20 )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "737c8d8d",
   "metadata": {},
   "source": [
    "# Runtime Comparisions: Orthographic Projection vs Induced Map\n",
    "In order for our induced method to train in a reasonable amount of time, we need to make sure that the evalutation time of the induced mapping is on par with the evaluation time of the orthgraphic projection method. Intitivly this should be possible because both methods are calculating spherical harmonic coeficents. However, e2nn is built to do efficent convolutions, not nessercarly integrals, so some creativity is required to write efficent code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c56c897",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Orthographic Projection timing\n",
    "start_time = time.time()\n",
    "x = torch.rand(batch_size,1,image_size,image_size)\n",
    "x = nn.GeometricTensor( x , feat_type_in  )\n",
    "y = SO2_conv_1.forward(x)\n",
    "z = orthographic_proj( y.tensor )\n",
    "stop_time = time.time()\n",
    "\n",
    "\n",
    "total_time =  stop_time - start_time\n",
    "print(\"Orthographic Projection Timing:\" , total_time)\n",
    "\n",
    "\n",
    "### Induced method I timing\n",
    "start_time = time.time()\n",
    "x = torch.rand(batch_size,1,image_size,image_size) \n",
    "x = nn.GeometricTensor( x , feat_type_in  )\n",
    "y = SO2_conv_1.forward(x)\n",
    "z = induce_I( y )\n",
    "stop_time = time.time()\n",
    "\n",
    "\n",
    "total_time =  stop_time - start_time\n",
    "\n",
    "print(\"Induced Map Method_I:\" , total_time)\n",
    "\n",
    "### Induced method II timing\n",
    "start_time = time.time()\n",
    "x = torch.rand(batch_size,1,image_size,image_size) \n",
    "x = nn.GeometricTensor( x , feat_type_in  )\n",
    "y = SO2_conv_1.forward(x)\n",
    "z = induce_II( y )\n",
    "stop_time = time.time()\n",
    "\n",
    "\n",
    "total_time =  stop_time - start_time\n",
    "print(\"Induced Map Method_II:\" , total_time)\n",
    "\n",
    "\n",
    "\n",
    "### Induced method III timing\n",
    "start_time = time.time()\n",
    "x = torch.rand(batch_size,1,image_size,image_size) \n",
    "x = nn.GeometricTensor( x , feat_type_in  )\n",
    "y = SO2_conv_1.forward(x)\n",
    "z = induce_III( y )\n",
    "stop_time = time.time()\n",
    "\n",
    "\n",
    "total_time =  stop_time - start_time\n",
    "print(\"Induced Map Method_III:\" , total_time)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c753a64",
   "metadata": {},
   "source": [
    "# Spherical Convolution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "29513f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Ask david if there is max beta that is optimal\n",
    "def s2_healpix_grid(rec_level: int=0, max_beta: float=np.pi/6):\n",
    "    \"\"\"Returns healpix grid up to a max_beta\n",
    "    \"\"\"\n",
    "    n_side = 2**rec_level\n",
    "    npix = hp.nside2npix(n_side)\n",
    "    m = hp.query_disc(nside=n_side, vec=(0,0,1), radius=max_beta)\n",
    "    beta, alpha = hp.pix2ang(n_side, m)\n",
    "    alpha = torch.from_numpy(alpha)\n",
    "    beta = torch.from_numpy(beta)\n",
    "    return torch.stack((alpha, beta)).float()\n",
    "\n",
    "def flat_wigner(lmax, alpha, beta, gamma):\n",
    "    return torch.cat([ (2 * l + 1) ** 0.5 * o3.wigner_D(l, alpha, beta, gamma).flatten(-2) for l in range(lmax + 1) ], dim=-1)\n",
    "\n",
    "### this should be changed,\n",
    "### or just set output to be equal to hidden so3 layer\n",
    "def s2_irreps(lmax):\n",
    "    return o3.Irreps([(1, (l, 1)) for l in range(lmax + 1)])\n",
    "\n",
    "def so3_irreps(lmax):\n",
    "    return o3.Irreps([(2 * l + 1, (l, 1)) for l in range(lmax + 1)])\n",
    "\n",
    "\n",
    "\n",
    "### defining convolution over sphere\n",
    "### just make sure that input so3 rep matches output so3 rep\n",
    "class S2Conv(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, f_in: int, f_out: int, lmax: int , kernel_grid: tuple):\n",
    "        super().__init__()\n",
    "\n",
    "        # filter weight parametrized over spatial grid on S2\n",
    "        self.register_parameter(\n",
    "          \"w\", torch.nn.Parameter(torch.randn(f_in, f_out, kernel_grid.shape[1]))\n",
    "        )  # [f_in, f_out, n_s2_pts]\n",
    "\n",
    "        # linear projection to convert filter weights to fourier domain\n",
    "        self.register_buffer(\n",
    "          \"Y\", o3.spherical_harmonics_alpha_beta(range(lmax + 1), *kernel_grid, normalization=\"component\")\n",
    "        )  # [n_s2_pts, (2*lmax+1)**2]\n",
    "\n",
    "\n",
    "        # defines group convolution using appropriate irreps\n",
    "        self.lin = o3.Linear( s2_irreps(lmax) , so3_irreps(lmax) ,  f_in=f_in, f_out=f_out, internal_weights=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        psi = torch.einsum(\"ni,xyn->xyi\", self.Y, self.w) / self.Y.shape[0] ** 0.5\n",
    "        return self.lin(x , weight=psi)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9e9405",
   "metadata": {},
   "source": [
    "# SO(3) Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e31a6e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def so3_healpix_grid(rec_level: int=3):\n",
    "    \"\"\"Returns healpix grid over so3 of equally spaced rotations\n",
    "   \n",
    "    https://github.com/google-research/google-research/blob/4808a726f4b126ea38d49cdd152a6bb5d42efdf0/implicit_pdf/models.py#L272\n",
    "    alpha: 0-2pi around Y\n",
    "    beta: 0-pi around X\n",
    "    gamma: 0-2pi around Y\n",
    "    rec_level | num_points | bin width (deg)\n",
    "    ----------------------------------------\n",
    "         0    |         72 |    60\n",
    "         1    |        576 |    30\n",
    "         2    |       4608 |    15\n",
    "         3    |      36864 |    7.5\n",
    "         4    |     294912 |    3.75\n",
    "         5    |    2359296 |    1.875\n",
    "         \n",
    "    :return: tensor of shape (3, npix)\n",
    "    \"\"\"\n",
    "    n_side = 2**rec_level\n",
    "    npix = hp.nside2npix(n_side)\n",
    "    beta, alpha = hp.pix2ang(n_side, torch.arange(npix))\n",
    "    gamma = torch.linspace(0, 2*np.pi, 6*n_side + 1)[:-1]\n",
    "\n",
    "    alpha = alpha.repeat(len(gamma))\n",
    "    beta = beta.repeat(len(gamma))\n",
    "    gamma = torch.repeat_interleave(gamma, npix)\n",
    "    return torch.stack((alpha, beta, gamma)).float()\n",
    "\n",
    "###convolutation over so3\n",
    "### maybe faster way to do this\n",
    "class SO3Conv(torch.nn.Module):\n",
    "    def __init__(self, f_in: int, f_out: int, lmax: int, kernel_grid: tuple):\n",
    "        super().__init__()\n",
    "\n",
    "        # filter weight parametrized over spatial grid on SO3\n",
    "        self.register_parameter(\n",
    "          \"w\", torch.nn.Parameter(torch.randn(f_in, f_out, kernel_grid.shape[1]))\n",
    "        )  # [f_in, f_out, n_so3_pts]\n",
    "\n",
    "        # wigner D matrices used to project spatial signal to irreps of SO(3)\n",
    "        self.register_buffer(\"D\", flat_wigner(lmax, *kernel_grid))  # [n_so3_pts, sum_l^L (2*l+1)**2]\n",
    "\n",
    "        # defines group convolution using appropriate irreps\n",
    "        self.lin = o3.Linear(so3_irreps(lmax), so3_irreps(lmax), f_in=f_in, f_out=f_out, internal_weights=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''Perform SO3 group convolution to produce signal over irreps of SO(3).\n",
    "        First project filter into fourier domain then perform convolution\n",
    "\n",
    "        :x: tensor of shape (B, f_in, sum_l^L (2*l+1)**2), signal over SO3 irreps\n",
    "        :return: tensor of shape (B, f_out, sum_l^L (2*l+1)**2)\n",
    "        '''\n",
    "        psi = torch.einsum(\"ni,xyn->xyi\", self.D, self.w) / self.D.shape[0] ** 0.5\n",
    "        return self.lin(x, weight=psi)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc2e915",
   "metadata": {},
   "source": [
    "# Loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbfe0a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_trace(rotA, rotB):\n",
    "    \n",
    "    '''\n",
    "    rotA, rotB are tensors of shape (*,3,3)\n",
    "    returns Tr(rotA, rotB.T)\n",
    "    '''\n",
    "    #rotA = rotA.type( torch.long )\n",
    "    #rotB = rotB.type( torch.long )\n",
    "    prod = torch.matmul(rotA, rotB.transpose(-1, -2))\n",
    "    trace = prod.diagonal(dim1=-1, dim2=-2).sum(-1)\n",
    "    return trace\n",
    "\n",
    "def rotation_error(rotA, rotB):\n",
    "    '''\n",
    "    rotA, rotB are tensors of shape (*,3,3)\n",
    "    returns rotation error in radians, tensor of shape (*)\n",
    "    '''\n",
    "    #rotA = rotA.type(torch.long)\n",
    "    #rotB = rotB.type(torch.long)\n",
    "    trace = compute_trace(rotA, rotB)\n",
    "    return torch.arccos(torch.clamp( (trace - 1)/2, -1, 1))\n",
    "\n",
    "def nearest_rotmat(src, target):\n",
    "    \n",
    "    '''return index of target that is nearest to each element in src\n",
    "    uses negative trace of the dot product to avoid arccos operation\n",
    "    :src: tensor of shape (B, 3, 3)\n",
    "    :target: tensor of shape (*, 3, 3)\n",
    "    '''\n",
    "    trace = compute_trace(src.unsqueeze(1), target.unsqueeze(0))\n",
    "   \n",
    "    return torch.max(trace, dim=1)[1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b40d37",
   "metadata": {},
   "source": [
    "# Defining the Standard I2S Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5004677b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### I2S network\n",
    "class I2S(torch.nn.Module):\n",
    "    \n",
    "    ### Instantiate I2S-style network for predicting distributions over SO(3) from\n",
    "    ### predictions made on single image using an induction layer \n",
    "    \n",
    "    def __init__(self, lmax=20 , kmax = 50 , image_size = 200 , so2_kernel_size = 25 ):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.lmax = lmax\n",
    "        self.kmax = kmax\n",
    "        self.image_size = image_size\n",
    "        self.kernel_size = so2_kernel_size\n",
    "\n",
    "        ### no image encoder, can add this later\n",
    "        ### self.encoder = ImageEncoder()\n",
    "\n",
    "        ### defining the SO2 action\n",
    "        SO2_act = gspaces.Rot2dOnR2(N=-1,maximum_frequency=self.kmax)\n",
    "\n",
    "        ### suppose that input is trival so2 rep\n",
    "        rep_in = [ SO2_act.irrep(0) ]\n",
    "\n",
    "        # ### specifiy the hiden SO2 layer muplicities\n",
    "        hidden_mulplicities_SO2 = { '0' : 1 , '1': 1 , '2':1 , '3':1  , '4':1   }\n",
    "        hidden_so2 = convert_SO2( hidden_mulplicities_SO2 )\n",
    "        \n",
    "        ### SO2 convolution layer\n",
    "        self.SO2_conv_1 = SO2_Convolution_Layer( rep_in = rep_in, rep_out = hidden_so2 ,k_max = self.kmax,  kernel_size=self.kernel_size )\n",
    "\n",
    "        ### the output mupliciteis of the induced SO3 layer\n",
    "        mulplicities_SO3 = { '0' :1 , '1' :1 , '2' : 1 , '3': 1 , '4':1  }\n",
    "\n",
    "        ##### the induction representation layer, \n",
    "        ### compute the number of output channels of hidden rep\n",
    "        channels_in = rep_ops.compute_SO2_dimension( hidden_mulplicities_SO2  )\n",
    "    \n",
    "        self.proj = Image2SphereProjector( fmap_shape=(channels_in ,image_size -kernel_size+1,image_size-kernel_size+1), sphere_fdim= 50, lmax=lmax,\n",
    "               coverage = 0.9,\n",
    "               sigma = 0.2,\n",
    "               max_beta = np.radians(90),\n",
    "               taper_beta = np.radians(75),\n",
    "               rec_level = 2,\n",
    "               n_subset = 20 )\n",
    "        \n",
    "        \n",
    "        ### output format is: batch, number of output channels, number of input channels\n",
    "        ### these are all in form of \n",
    "        s2_kernel_grid = s2_healpix_grid(max_beta=np.inf, rec_level=1)\n",
    "\n",
    "        ### THIS IS L_MAX - 1 !!! Need to standardize notations\n",
    "        ### compute the dimension of s2 input features\n",
    "        f_in = rep_ops.compute_SO3_dimension( mulplicities_SO3 )\n",
    "        self.s2_conv = S2Conv( f_in = f_in , f_out= 105 , lmax=self.lmax-1 , kernel_grid = s2_kernel_grid )\n",
    "\n",
    "        #### also L_max - 1 !!! Need to standardize notations\n",
    "        so3_kernel_grid = so3_healpix_grid(rec_level=3)\n",
    "        ### output is one dimensional so can use logits \n",
    "        self.so3_conv = SO3Conv( f_in = 105 , f_out=1 , lmax=self.lmax-1 , kernel_grid = so3_kernel_grid )\n",
    "        self.so3_act = e3nn.nn.SO3Activation( self.lmax-1 , self.lmax-1 , act=torch.relu, resolution=20)\n",
    "        \n",
    "        output_xyx = so3_healpix_grid(rec_level=2)\n",
    "        self.register_buffer( \"output_wigners\", flat_wigner( self.lmax - 1 , *output_xyx).transpose(0,1) )\n",
    "        self.register_buffer( \"output_rotmats\", o3.angles_to_matrix(*output_xyx) )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        ###'''Returns so3 irreps\n",
    "        ###:x: the input image, tensor of shape (B, 1, image_size, image_size)\n",
    "        ## x must be a geometric tensor\n",
    "\n",
    "        x = self.SO2_conv_1( x )\n",
    "        x = self.proj( x.tensor )\n",
    "        x = self.s2_conv( x )\n",
    "        x = self.so3_act( x )\n",
    "        x = self.so3_conv( x )\n",
    "    \n",
    "        return x\n",
    "  \n",
    "    def compute_loss(self, img, gt_rot):\n",
    "        ###'''Compute cross entropy loss using ground truth rotation, the correct label\n",
    "        ###is the nearest rotation in the spatial grid to the ground truth rotation\n",
    "\n",
    "        ### :img: float tensor of shape (B, 3, 224, 224)\n",
    "        ### :gt_rotation: valid rotation matrices, tensor of shape (B, 3, 3)\n",
    "        \n",
    "        ### run image through network\n",
    "        x = self.forward(img)\n",
    "        \n",
    "        ### make sure output is long type tensor\n",
    "        # x = x.tensor\n",
    "        \n",
    "        grid_signal = torch.matmul(x, self.output_wigners ).squeeze(1)\n",
    "        rotmats = self.output_rotmats\n",
    "\n",
    "\n",
    "        # find nearest grid point to ground truth rotation matrix\n",
    "        rot_id = nearest_rotmat( gt_rot , rotmats )\n",
    "            \n",
    "        loss = torch.nn.CrossEntropyLoss()( grid_signal.type( torch.float )  , rot_id  )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pred_id = grid_signal.max(dim=1)[1]\n",
    "            pred_rotmat = rotmats[pred_id]\n",
    "            acc = rotation_error(gt_rot, pred_rotmat)\n",
    "\n",
    "        return loss, acc.cpu().numpy()\n",
    "   \n",
    "    @torch.no_grad()\n",
    "    def compute_probabilities(self, img, wigners):\n",
    "        x = self.forward(img)\n",
    "        logits = torch.matmul(x, wigners).squeeze(1)\n",
    "        return torch.nn.Softmax(dim=1)(logits)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a32c693",
   "metadata": {},
   "source": [
    "# Defining the Induced Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69233a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Induced_I2S network\n",
    "class Induced_I2S(torch.nn.Module):\n",
    "    \n",
    "    ### Instantiate I2S-style network for predicting distributions over SO(3) from\n",
    "    ### predictions made on single image using an induction layer \n",
    "    \n",
    "    def __init__(self, lmax=20 , kmax = 50 , image_size = 200 , so2_kernel_size = 25 ):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.lmax = lmax\n",
    "        self.kmax = kmax\n",
    "        self.image_size = image_size\n",
    "        self.kernel_size = so2_kernel_size\n",
    "\n",
    "        ### no image encoder, can add this later\n",
    "        ### self.encoder = ImageEncoder()\n",
    "\n",
    "        ### defining the SO2 action\n",
    "        SO2_act = gspaces.Rot2dOnR2(N=-1,maximum_frequency=self.kmax)\n",
    "\n",
    "        ### suppose that input is trival so2 rep\n",
    "        rep_in = [ SO2_act.irrep(0) ]\n",
    "\n",
    "        # ### specifiy the hiden SO2 layer muplicities\n",
    "        hidden_mulplicities_SO2 = { '0' : 1 , '1': 1 , '2':1 , '3':1  , '4':1   }\n",
    "        hidden_so2 = convert_SO2( hidden_mulplicities_SO2 )\n",
    "        \n",
    "        ### SO2 convolution layer\n",
    "        self.SO2_conv_1 = SO2_Convolution_Layer( rep_in = rep_in, rep_out = hidden_so2 ,k_max = self.kmax,  kernel_size=self.kernel_size )\n",
    "\n",
    "        ### the output mupliciteis of the induced SO3 layer\n",
    "        mulplicities_SO3 = { '0' :1 , '1' :1 , '2' : 1 , '3': 1 , '4':1  }\n",
    "\n",
    "        ##### the induction representation layer, \n",
    "        ### compute the number of output channels of hidden rep\n",
    "        channels_in = rep_ops.compute_SO2_dimension( hidden_mulplicities_SO2  )\n",
    "        self.induce = Induction_Layer_II( channels = channels_in, image_shape=(self.image_size - self.kernel_size +1) , k_max =self.kmax, L_max=self.lmax , dict_rep_in = hidden_mulplicities_SO2 , dict_rep_out = mulplicities_SO3 )\n",
    "\n",
    "        ### output format is: batch, number of output channels, number of input channels\n",
    "        ### these are all in form of \n",
    "        s2_kernel_grid = s2_healpix_grid(max_beta=np.inf, rec_level=1)\n",
    "\n",
    "        ### THIS IS L_MAX - 1 !!! Need to standardize notations\n",
    "        ### compute the dimension of s2 input features\n",
    "        f_in = rep_ops.compute_SO3_dimension( mulplicities_SO3 )\n",
    "        self.s2_conv = S2Conv( f_in = f_in , f_out= 105 , lmax=self.lmax-1 , kernel_grid = s2_kernel_grid )\n",
    "\n",
    "        #### also L_max - 1 !!! Need to standardize notations\n",
    "        so3_kernel_grid = so3_healpix_grid(rec_level=3)\n",
    "        ### output is one dimensional so can use logits \n",
    "        self.so3_conv = SO3Conv( f_in = 105 , f_out=1 , lmax=self.lmax-1 , kernel_grid = so3_kernel_grid )\n",
    "        self.so3_act = e3nn.nn.SO3Activation( self.lmax-1 , self.lmax-1 , act=torch.relu, resolution=20)\n",
    "        \n",
    "        output_xyx = so3_healpix_grid(rec_level=2)\n",
    "        self.register_buffer( \"output_wigners\", flat_wigner( self.lmax - 1 , *output_xyx).transpose(0,1) )\n",
    "        self.register_buffer( \"output_rotmats\", o3.angles_to_matrix(*output_xyx) )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        ###'''Returns so3 irreps\n",
    "        ###:x: the input image, tensor of shape (B, 1, image_size, image_size)\n",
    "        ## x must be a geometric tensor\n",
    "\n",
    "        x = self.SO2_conv_1(x)\n",
    "        x = self.induce( x )\n",
    "        x = self.s2_conv( x )\n",
    "        x = self.so3_act( x )\n",
    "        x = self.so3_conv( x )\n",
    "    \n",
    "        return x\n",
    "  \n",
    "    def compute_loss(self, img, gt_rot):\n",
    "        ###'''Compute cross entropy loss using ground truth rotation, the correct label\n",
    "        ###is the nearest rotation in the spatial grid to the ground truth rotation\n",
    "\n",
    "        ### :img: float tensor of shape (B, 3, 224, 224)\n",
    "        ### :gt_rotation: valid rotation matrices, tensor of shape (B, 3, 3)\n",
    "        \n",
    "        ### run image through network\n",
    "        x = self.forward(img)\n",
    "        \n",
    "        ### make sure output is long type tensor\n",
    "        # x = x.tensor\n",
    "        \n",
    "        grid_signal = torch.matmul(x, self.output_wigners ).squeeze(1)\n",
    "        rotmats = self.output_rotmats\n",
    "\n",
    "\n",
    "        # find nearest grid point to ground truth rotation matrix\n",
    "        rot_id = nearest_rotmat( gt_rot , rotmats )\n",
    "            \n",
    "        loss = torch.nn.CrossEntropyLoss()( grid_signal.type( torch.float )  , rot_id  )\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            pred_id = grid_signal.max(dim=1)[1]\n",
    "            pred_rotmat = rotmats[pred_id]\n",
    "            acc = rotation_error(gt_rot, pred_rotmat)\n",
    "\n",
    "        return loss, acc.cpu().numpy()\n",
    "   \n",
    "    @torch.no_grad()\n",
    "    def compute_probabilities(self, img, wigners):\n",
    "        x = self.forward(img)\n",
    "        logits = torch.matmul(x, wigners).squeeze(1)\n",
    "        return torch.nn.Softmax(dim=1)(logits)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f757c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lmax=4\n",
    "induced_arch = Induced_I2S( lmax=lmax , kmax = 50 , image_size = 120 , so2_kernel_size=65 )\n",
    "\n",
    "output_xyx = so3_healpix_grid(rec_level=3) # 37K points\n",
    "output_wigners = flat_wigner( lmax - 1 , *output_xyx).transpose(0, 1)\n",
    "output_rotmats = o3.angles_to_matrix(*output_xyx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47fa21b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_i2s = I2S( lmax=lmax , kmax = 50 , image_size = 120 , so2_kernel_size = 65 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf50e052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print( induced_arch )\n",
    "# print( standard_i2s )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03468f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_so3_distribution(probs: torch.Tensor,\n",
    "                          rots: torch.Tensor,\n",
    "                          gt_rotation=None,\n",
    "                          fig=None,\n",
    "                          ax=None,\n",
    "                          display_threshold_probability=0.000005,\n",
    "                          show_color_wheel: bool=True,\n",
    "                          canonical_rotation=torch.eye(3),\n",
    "                         ):\n",
    "    '''\n",
    "    Taken from https://github.com/google-research/google-research/blob/master/implicit_pdf/evaluation.py\n",
    "    '''\n",
    "    cmap = plt.cm.hsv\n",
    "\n",
    "    def _show_single_marker(ax, rotation, marker, edgecolors=True, facecolors=False):\n",
    "        alpha, beta, gamma = o3.matrix_to_angles(rotation)\n",
    "        color = cmap(0.5 + gamma.repeat(2) / 2. / np.pi)[-1]\n",
    "        ax.scatter(alpha, beta-np.pi/2, s=2000, edgecolors=color, facecolors='none', marker=marker, linewidth=5)\n",
    "        ax.scatter(alpha, beta-np.pi/2, s=1500, edgecolors='k', facecolors='none', marker=marker, linewidth=2)\n",
    "        ax.scatter(alpha, beta-np.pi/2, s=2500, edgecolors='k', facecolors='none', marker=marker, linewidth=2)\n",
    "\n",
    "    if ax is None:\n",
    "        fig = plt.figure(figsize=(8, 4), dpi=200)\n",
    "        fig.subplots_adjust(0.01, 0.08, 0.90, 0.95)\n",
    "        ax = fig.add_subplot(111, projection='mollweide')\n",
    "\n",
    "    rots = rots @ canonical_rotation\n",
    "    scatterpoint_scaling = 3e3\n",
    "    alpha, beta, gamma = o3.matrix_to_angles(rots)\n",
    "\n",
    "    # offset alpha and beta so different gammas are visible\n",
    "    R = 0.02\n",
    "    alpha += R * np.cos(gamma)\n",
    "    beta += R * np.sin(gamma)\n",
    "\n",
    "    which_to_display = (probs > display_threshold_probability)\n",
    "\n",
    "    # Display the distribution\n",
    "    ax.scatter(alpha[which_to_display],\n",
    "               beta[which_to_display]-np.pi/2,\n",
    "               s=scatterpoint_scaling * probs[which_to_display],\n",
    "               c=cmap(0.5 + gamma[which_to_display] / 2. / np.pi))\n",
    "    if gt_rotation is not None:\n",
    "        if len(gt_rotation.shape) == 2:\n",
    "            gt_rotation = gt_rotation.unsqueeze(0)\n",
    "        gt_rotation = gt_rotation @ canonical_rotation\n",
    "        _show_single_marker(ax, gt_rotation, 'o')\n",
    "    ax.grid()\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "\n",
    "    if show_color_wheel:\n",
    "        # Add a color wheel showing the tilt angle to color conversion.\n",
    "        ax = fig.add_axes([0.86, 0.17, 0.12, 0.12], projection='polar')\n",
    "        theta = np.linspace(-3 * np.pi / 2, np.pi / 2, 200)\n",
    "        radii = np.linspace(0.4, 0.5, 2)\n",
    "        _, theta_grid = np.meshgrid(radii, theta)\n",
    "        colormap_val = 0.5 + theta_grid / np.pi / 2.\n",
    "        ax.pcolormesh(theta, radii, colormap_val.T, cmap=cmap)\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_xticklabels([r'90$\\degree$', None,\n",
    "                            r'180$\\degree$', None,\n",
    "                            r'270$\\degree$', None,\n",
    "                            r'0$\\degree$'], fontsize=14)\n",
    "        ax.spines['polar'].set_visible(False)\n",
    "        plt.text(0.5, 0.5, 'Tilt', fontsize=14,\n",
    "                 horizontalalignment='center',\n",
    "                 verticalalignment='center', transform=ax.transAxes)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dbd4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, labels_file, img_dir ):\n",
    "        \n",
    "        with open( labels_file , 'rb') as handle:\n",
    "            self.img_labels = pickle.load(handle)\n",
    "            \n",
    "        self.img_dir = img_dir \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len( self.img_labels.keys()  ) \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        file_path = self.img_dir +'img_num:'+str(idx) +'.npy'\n",
    "        image = np.load( file_path, allow_pickle=True )\n",
    "        image = np.float32(image)\n",
    "        image = torch.from_numpy(image)\n",
    "\n",
    "        label = self.img_labels[ str(idx) ]\n",
    "        label = np.float32(label)\n",
    "        label = torch.from_numpy(label)\n",
    "        \n",
    "        \n",
    "\n",
    "        return image, label\n",
    "\n",
    "        \n",
    "        \n",
    "img_dir = './data/image_data/'\n",
    "labels_file = './data/image_data/rotations.pickle'\n",
    "\n",
    "data_set = CustomImageDataset( labels_file = labels_file,  img_dir =img_dir )\n",
    "train_dataloader = DataLoader( data_set, batch_size=1, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d3120a",
   "metadata": {},
   "source": [
    "# Training the Induced_I2S Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e91019",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD( induced_arch.parameters(), lr=0.003, momentum=0.03  )\n",
    "\n",
    "rep_in = [ SO2_act.irrep(0) ]\n",
    "feat_type_in = nn.FieldType( SO2_act, rep_in  )\n",
    "\n",
    "num_epoch = 3\n",
    "for epoch in range(num_epoch):\n",
    "    for img, label in train_dataloader:\n",
    "    \n",
    "        print(img.shape)\n",
    "\n",
    "        img = torch.unsqueeze(img, 1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        x = nn.GeometricTensor( img , feat_type_in  )\n",
    "       \n",
    "        loss , a = induced_arch.compute_loss( x , label ) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### print model parameters\n",
    "        \n",
    "        print(loss)\n",
    "        \n",
    "    print()\n",
    "    print('Epoch Number:' , epoch  )\n",
    "    print(\"Training Loss:\" , loss )\n",
    "    print()\n",
    "\n",
    "        \n",
    "\n",
    "### save model to file\n",
    "file = 'Induced_I2S_model.pt'\n",
    "torch.save(induced_arch, PATH)\n",
    "\n",
    "### post training\n",
    "test_dataloader = DataLoader( data_set, batch_size=1, shuffle=True)\n",
    "with torch.no_grad():\n",
    "    for img, label in train_dataloader:\n",
    "        \n",
    "        img = torch.unsqueeze(img, 1)  \n",
    "        x = nn.GeometricTensor( img , feat_type_in  )\n",
    "        y = induced_arch.forward(x)\n",
    "        \n",
    "        \n",
    "        logits = torch.matmul(y, output_wigners).squeeze(1)\n",
    "        probs = torch.nn.Softmax(dim=1)(logits)        \n",
    "\n",
    "        plot_so3_distribution(probs[0], output_rotmats, gt_rotation=label)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3246bb",
   "metadata": {},
   "source": [
    "# Training the I2S Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af949c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD( standard_i2s.parameters(), lr=0.003, momentum=0.03  )\n",
    "\n",
    "rep_in = [ SO2_act.irrep(0) ]\n",
    "feat_type_in = nn.FieldType( SO2_act, rep_in  )\n",
    "\n",
    "num_epoch = 3\n",
    "for epoch in range(num_epoch):\n",
    "    for img, label in train_dataloader:\n",
    "    \n",
    "        print(img.shape)\n",
    "\n",
    "        img = torch.unsqueeze(img, 1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        x = nn.GeometricTensor( img , feat_type_in  )\n",
    "       \n",
    "        loss , a = standard_i2s.compute_loss( x , label ) \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        ### print model parameters\n",
    "        \n",
    "        print(loss)\n",
    "        \n",
    "    print()\n",
    "    print('Epoch Number:' , epoch  )\n",
    "    print(\"Training Loss:\" , loss )\n",
    "    print()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b528bf6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b4deea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33b1920",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
